model:
  arch: mllama-vllm-generate-1
  model_type: pretrain_mllama_vllm_generate_1
  use_decoder_only_language_model: False
  mllama_pretrained_model_name_or_path: "Qwen/Qwen2-VL-7B-Instruct"
  text_pretrained_model_name_or_path: "google/flan-t5-xxl"
  freeze_mllama: True
  freeze_language: True
  dtype: "bfloat16"
  max_txt_len: 256
  mm_projector_type: "mlp2x_gelu_t5_norm"
  layer_norm_reinit_weight_with_language_encoder: True
  text_input_key: "answers"

  vllm_config:
    max_model_len: 8192
    # max_num_batched_tokens: 737280
    # max_num_batched_tokens: 491520
    max_num_batched_tokens: 131072
    max_num_seqs: 2048
    # max_num_seqs: 128
    gpu_memory_utilization: 0.9
    temperature: 0.6
    top_p: 0.9
    max_tokens: 256
    min_tokens: 1
    ignore_eos: False
    embedding_layer_name: "model.norm"
    enforce_eager: True
    tensor_parallel_size: 4
    return_hidden_states: True

datasets:
  cc_sbu_mllama_vllm_process_wids:
    batch_size: 8192
    build_info:
      storage: minigpt4/configs/datasets/cc_sbu_mllama_vllm_process_wids/wids_shards.json

run:
  task: image_text_process_data
  runner: runner_process_data
  # optimizer
  lr_sched: "linear_warmup_cosine_lr"
  init_lr: 1e-4
  min_lr: 8e-5
  warmup_lr: 1e-6
  use_clip_grad_norm: False
  max_grad_norm: 1.0
  output_shard_path: ["/your/path/to/qwen2_vl_embed_ccsbu_debug", "%06d.tar", 0]

  weight_decay: 0.05
  max_epoch: 40
  num_workers: 32
  warmup_steps: 2000
  iters_per_epoch: 5000
  log_freq: 1

  seed: 42
  output_dir: "./run_qwen2_vl_embed_ccsbu_debug"

  amp: True
  amp_dtype: "bfloat16"
  resume_ckpt_path: null

  find_unused_parameters: False

  evaluate: False 
  train_splits: ["train"]
  # valid_splits: ["eval"]

  device: "cuda"
  world_size: 1
  dist_url: "env://"
  distributed: False

  wandb_log: True
  job_name: run_qwen2_vl_embed_ccsbu_debug
  wandb_project_name: run_qwen2_vl_embed_ccsbu_debug